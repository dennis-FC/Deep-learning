{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5) # 3=>input channel 6=>output channel 5=>kernal_size\n",
    "        self.pool = nn.MaxPool2d(2,2) # Maxpooling(2*2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5) # layer2 6=>input channel 16=>output channel 5=>kernal size\n",
    "        self.fc1 = nn.Linear(16*5*5,120) \n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,3) # 3 dimension output = good bad none\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*5*5) #拉平\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "cnn = CNN()\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9) # Gradient Descent learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.908\n",
      "[1,   200] loss: 0.666\n",
      "[1,   300] loss: 0.739\n",
      "[1,   400] loss: 0.608\n",
      "[1,   500] loss: 0.820\n",
      "[1,   600] loss: 0.673\n",
      "[1,   700] loss: 0.429\n",
      "[1,   800] loss: 0.571\n",
      "[1,   900] loss: 0.520\n",
      "[1,  1000] loss: 0.519\n",
      "[1,  1100] loss: 0.641\n",
      "[1,  1200] loss: 0.341\n",
      "[1,  1300] loss: 0.809\n",
      "[1,  1400] loss: 0.507\n",
      "[1,  1500] loss: 0.623\n",
      "[1,  1600] loss: 0.627\n",
      "[1,  1700] loss: 0.486\n",
      "[1,  1800] loss: 0.481\n",
      "[1,  1900] loss: 0.340\n",
      "[1,  2000] loss: 0.286\n",
      "[1,  2100] loss: 0.432\n",
      "[1,  2200] loss: 0.669\n",
      "[1,  2300] loss: 0.623\n",
      "[1,  2400] loss: 0.663\n",
      "[1,  2500] loss: 0.560\n",
      "[1,  2600] loss: 0.548\n",
      "[1,  2700] loss: 0.461\n",
      "[1,  2800] loss: 0.368\n",
      "[1,  2900] loss: 0.424\n",
      "[1,  3000] loss: 0.366\n",
      "[1,  3100] loss: 0.633\n",
      "[1,  3200] loss: 0.972\n",
      "[1,  3300] loss: 0.710\n",
      "[1,  3400] loss: 0.599\n",
      "[1,  3500] loss: 0.607\n",
      "[2,   100] loss: 0.605\n",
      "[2,   200] loss: 0.564\n",
      "[2,   300] loss: 0.635\n",
      "[2,   400] loss: 0.520\n",
      "[2,   500] loss: 0.690\n",
      "[2,   600] loss: 0.607\n",
      "[2,   700] loss: 0.442\n",
      "[2,   800] loss: 0.550\n",
      "[2,   900] loss: 0.442\n",
      "[2,  1000] loss: 0.511\n",
      "[2,  1100] loss: 0.617\n",
      "[2,  1200] loss: 0.240\n",
      "[2,  1300] loss: 0.576\n",
      "[2,  1400] loss: 0.356\n",
      "[2,  1500] loss: 0.396\n",
      "[2,  1600] loss: 0.475\n",
      "[2,  1700] loss: 0.338\n",
      "[2,  1800] loss: 0.301\n",
      "[2,  1900] loss: 0.195\n",
      "[2,  2000] loss: 0.206\n",
      "[2,  2100] loss: 0.312\n",
      "[2,  2200] loss: 0.535\n",
      "[2,  2300] loss: 0.386\n",
      "[2,  2400] loss: 0.352\n",
      "[2,  2500] loss: 0.313\n",
      "[2,  2600] loss: 0.239\n",
      "[2,  2700] loss: 0.247\n",
      "[2,  2800] loss: 0.137\n",
      "[2,  2900] loss: 0.086\n",
      "[2,  3000] loss: 0.154\n",
      "[2,  3100] loss: 0.402\n",
      "[2,  3200] loss: 0.438\n",
      "[2,  3300] loss: 0.134\n",
      "[2,  3400] loss: 0.138\n",
      "[2,  3500] loss: 0.319\n",
      "[3,   100] loss: 0.157\n",
      "[3,   200] loss: 0.214\n",
      "[3,   300] loss: 0.342\n",
      "[3,   400] loss: 0.296\n",
      "[3,   500] loss: 0.517\n",
      "[3,   600] loss: 0.187\n",
      "[3,   700] loss: 0.108\n",
      "[3,   800] loss: 0.294\n",
      "[3,   900] loss: 0.113\n",
      "[3,  1000] loss: 0.232\n",
      "[3,  1100] loss: 0.286\n",
      "[3,  1200] loss: 0.135\n",
      "[3,  1300] loss: 0.194\n",
      "[3,  1400] loss: 0.239\n",
      "[3,  1500] loss: 0.217\n",
      "[3,  1600] loss: 0.320\n",
      "[3,  1700] loss: 0.239\n",
      "[3,  1800] loss: 0.175\n",
      "[3,  1900] loss: 0.125\n",
      "[3,  2000] loss: 0.117\n",
      "[3,  2100] loss: 0.310\n",
      "[3,  2200] loss: 0.188\n",
      "[3,  2300] loss: 0.253\n",
      "[3,  2400] loss: 0.212\n",
      "[3,  2500] loss: 0.283\n",
      "[3,  2600] loss: 0.197\n",
      "[3,  2700] loss: 0.201\n",
      "[3,  2800] loss: 0.087\n",
      "[3,  2900] loss: 0.062\n",
      "[3,  3000] loss: 0.087\n",
      "[3,  3100] loss: 0.206\n",
      "[3,  3200] loss: 0.246\n",
      "[3,  3300] loss: 0.235\n",
      "[3,  3400] loss: 0.131\n",
      "[3,  3500] loss: 0.261\n",
      "[4,   100] loss: 0.107\n",
      "[4,   200] loss: 0.161\n",
      "[4,   300] loss: 0.269\n",
      "[4,   400] loss: 0.372\n",
      "[4,   500] loss: 0.328\n",
      "[4,   600] loss: 0.189\n",
      "[4,   700] loss: 0.099\n",
      "[4,   800] loss: 0.281\n",
      "[4,   900] loss: 0.108\n",
      "[4,  1000] loss: 0.226\n",
      "[4,  1100] loss: 0.245\n",
      "[4,  1200] loss: 0.170\n",
      "[4,  1300] loss: 0.195\n",
      "[4,  1400] loss: 0.197\n",
      "[4,  1500] loss: 0.191\n",
      "[4,  1600] loss: 0.255\n",
      "[4,  1700] loss: 0.251\n",
      "[4,  1800] loss: 0.190\n",
      "[4,  1900] loss: 0.110\n",
      "[4,  2000] loss: 0.119\n",
      "[4,  2100] loss: 0.306\n",
      "[4,  2200] loss: 0.181\n",
      "[4,  2300] loss: 0.235\n",
      "[4,  2400] loss: 0.216\n",
      "[4,  2500] loss: 0.269\n",
      "[4,  2600] loss: 0.197\n",
      "[4,  2700] loss: 0.292\n",
      "[4,  2800] loss: 0.180\n",
      "[4,  2900] loss: 0.136\n",
      "[4,  3000] loss: 0.219\n",
      "[4,  3100] loss: 0.194\n",
      "[4,  3200] loss: 0.265\n",
      "[4,  3300] loss: 0.178\n",
      "[4,  3400] loss: 0.192\n",
      "[4,  3500] loss: 0.233\n",
      "[5,   100] loss: 0.116\n",
      "[5,   200] loss: 0.168\n",
      "[5,   300] loss: 0.267\n",
      "[5,   400] loss: 0.304\n",
      "[5,   500] loss: 0.379\n",
      "[5,   600] loss: 0.186\n",
      "[5,   700] loss: 0.080\n",
      "[5,   800] loss: 0.265\n",
      "[5,   900] loss: 0.095\n",
      "[5,  1000] loss: 0.195\n",
      "[5,  1100] loss: 0.223\n",
      "[5,  1200] loss: 0.111\n",
      "[5,  1300] loss: 0.136\n",
      "[5,  1400] loss: 0.204\n",
      "[5,  1500] loss: 0.306\n",
      "[5,  1600] loss: 0.231\n",
      "[5,  1700] loss: 0.219\n",
      "[5,  1800] loss: 0.168\n",
      "[5,  1900] loss: 0.116\n",
      "[5,  2000] loss: 0.095\n",
      "[5,  2100] loss: 0.364\n",
      "[5,  2200] loss: 0.199\n",
      "[5,  2300] loss: 0.226\n",
      "[5,  2400] loss: 0.234\n",
      "[5,  2500] loss: 0.265\n",
      "[5,  2600] loss: 0.176\n",
      "[5,  2700] loss: 0.214\n",
      "[5,  2800] loss: 0.090\n",
      "[5,  2900] loss: 0.058\n",
      "[5,  3000] loss: 0.073\n",
      "[5,  3100] loss: 0.177\n",
      "[5,  3200] loss: 0.234\n",
      "[5,  3300] loss: 0.123\n",
      "[5,  3400] loss: 0.101\n",
      "[5,  3500] loss: 0.173\n",
      "[6,   100] loss: 0.094\n",
      "[6,   200] loss: 0.174\n",
      "[6,   300] loss: 0.251\n",
      "[6,   400] loss: 0.424\n",
      "[6,   500] loss: 0.265\n",
      "[6,   600] loss: 0.162\n",
      "[6,   700] loss: 0.093\n",
      "[6,   800] loss: 0.244\n",
      "[6,   900] loss: 0.115\n",
      "[6,  1000] loss: 0.239\n",
      "[6,  1100] loss: 0.233\n",
      "[6,  1200] loss: 0.147\n",
      "[6,  1300] loss: 0.167\n",
      "[6,  1400] loss: 0.191\n",
      "[6,  1500] loss: 0.241\n",
      "[6,  1600] loss: 0.241\n",
      "[6,  1700] loss: 0.204\n",
      "[6,  1800] loss: 0.177\n",
      "[6,  1900] loss: 0.108\n",
      "[6,  2000] loss: 0.093\n",
      "[6,  2100] loss: 0.377\n",
      "[6,  2200] loss: 0.210\n",
      "[6,  2300] loss: 0.222\n",
      "[6,  2400] loss: 0.230\n",
      "[6,  2500] loss: 0.241\n",
      "[6,  2600] loss: 0.157\n",
      "[6,  2700] loss: 0.209\n",
      "[6,  2800] loss: 0.077\n",
      "[6,  2900] loss: 0.064\n",
      "[6,  3000] loss: 0.069\n",
      "[6,  3100] loss: 0.183\n",
      "[6,  3200] loss: 0.283\n",
      "[6,  3300] loss: 0.120\n",
      "[6,  3400] loss: 0.116\n",
      "[6,  3500] loss: 0.181\n",
      "[7,   100] loss: 0.124\n",
      "[7,   200] loss: 0.159\n",
      "[7,   300] loss: 0.245\n",
      "[7,   400] loss: 0.375\n",
      "[7,   500] loss: 0.272\n",
      "[7,   600] loss: 0.179\n",
      "[7,   700] loss: 0.089\n",
      "[7,   800] loss: 0.229\n",
      "[7,   900] loss: 0.111\n",
      "[7,  1000] loss: 0.197\n",
      "[7,  1100] loss: 0.232\n",
      "[7,  1200] loss: 0.091\n",
      "[7,  1300] loss: 0.146\n",
      "[7,  1400] loss: 0.136\n",
      "[7,  1500] loss: 0.219\n",
      "[7,  1600] loss: 0.241\n",
      "[7,  1700] loss: 0.171\n",
      "[7,  1800] loss: 0.143\n",
      "[7,  1900] loss: 0.113\n",
      "[7,  2000] loss: 0.061\n",
      "[7,  2100] loss: 0.338\n",
      "[7,  2200] loss: 0.157\n",
      "[7,  2300] loss: 0.218\n",
      "[7,  2400] loss: 0.222\n",
      "[7,  2500] loss: 0.220\n",
      "[7,  2600] loss: 0.143\n",
      "[7,  2700] loss: 0.165\n",
      "[7,  2800] loss: 0.069\n",
      "[7,  2900] loss: 0.031\n",
      "[7,  3000] loss: 0.070\n",
      "[7,  3100] loss: 0.168\n",
      "[7,  3200] loss: 0.208\n",
      "[7,  3300] loss: 0.135\n",
      "[7,  3400] loss: 0.148\n",
      "[7,  3500] loss: 0.168\n",
      "[8,   100] loss: 0.107\n",
      "[8,   200] loss: 0.157\n",
      "[8,   300] loss: 0.273\n",
      "[8,   400] loss: 0.407\n",
      "[8,   500] loss: 0.254\n",
      "[8,   600] loss: 0.180\n",
      "[8,   700] loss: 0.086\n",
      "[8,   800] loss: 0.231\n",
      "[8,   900] loss: 0.099\n",
      "[8,  1000] loss: 0.164\n",
      "[8,  1100] loss: 0.227\n",
      "[8,  1200] loss: 0.115\n",
      "[8,  1300] loss: 0.141\n",
      "[8,  1400] loss: 0.141\n",
      "[8,  1500] loss: 0.232\n",
      "[8,  1600] loss: 0.238\n",
      "[8,  1700] loss: 0.161\n",
      "[8,  1800] loss: 0.140\n",
      "[8,  1900] loss: 0.107\n",
      "[8,  2000] loss: 0.069\n",
      "[8,  2100] loss: 0.311\n",
      "[8,  2200] loss: 0.146\n",
      "[8,  2300] loss: 0.202\n",
      "[8,  2400] loss: 0.208\n",
      "[8,  2500] loss: 0.181\n",
      "[8,  2600] loss: 0.132\n",
      "[8,  2700] loss: 0.170\n",
      "[8,  2800] loss: 0.074\n",
      "[8,  2900] loss: 0.034\n",
      "[8,  3000] loss: 0.063\n",
      "[8,  3100] loss: 0.191\n",
      "[8,  3200] loss: 0.231\n",
      "[8,  3300] loss: 0.116\n",
      "[8,  3400] loss: 0.154\n",
      "[8,  3500] loss: 0.174\n",
      "[9,   100] loss: 0.107\n",
      "[9,   200] loss: 0.162\n",
      "[9,   300] loss: 0.244\n",
      "[9,   400] loss: 0.429\n",
      "[9,   500] loss: 0.255\n",
      "[9,   600] loss: 0.168\n",
      "[9,   700] loss: 0.100\n",
      "[9,   800] loss: 0.212\n",
      "[9,   900] loss: 0.100\n",
      "[9,  1000] loss: 0.179\n",
      "[9,  1100] loss: 0.194\n",
      "[9,  1200] loss: 0.108\n",
      "[9,  1300] loss: 0.141\n",
      "[9,  1400] loss: 0.115\n",
      "[9,  1500] loss: 0.234\n",
      "[9,  1600] loss: 0.219\n",
      "[9,  1700] loss: 0.196\n",
      "[9,  1800] loss: 0.149\n",
      "[9,  1900] loss: 0.100\n",
      "[9,  2000] loss: 0.068\n",
      "[9,  2100] loss: 0.287\n",
      "[9,  2200] loss: 0.132\n",
      "[9,  2300] loss: 0.185\n",
      "[9,  2400] loss: 0.212\n",
      "[9,  2500] loss: 0.153\n",
      "[9,  2600] loss: 0.113\n",
      "[9,  2700] loss: 0.165\n",
      "[9,  2800] loss: 0.069\n",
      "[9,  2900] loss: 0.024\n",
      "[9,  3000] loss: 0.052\n",
      "[9,  3100] loss: 0.167\n",
      "[9,  3200] loss: 0.203\n",
      "[9,  3300] loss: 0.131\n",
      "[9,  3400] loss: 0.135\n",
      "[9,  3500] loss: 0.182\n",
      "[10,   100] loss: 0.114\n",
      "[10,   200] loss: 0.150\n",
      "[10,   300] loss: 0.235\n",
      "[10,   400] loss: 0.375\n",
      "[10,   500] loss: 0.233\n",
      "[10,   600] loss: 0.171\n",
      "[10,   700] loss: 0.118\n",
      "[10,   800] loss: 0.248\n",
      "[10,   900] loss: 0.091\n",
      "[10,  1000] loss: 0.156\n",
      "[10,  1100] loss: 0.204\n",
      "[10,  1200] loss: 0.135\n",
      "[10,  1300] loss: 0.117\n",
      "[10,  1400] loss: 0.131\n",
      "[10,  1500] loss: 0.241\n",
      "[10,  1600] loss: 0.167\n",
      "[10,  1700] loss: 0.227\n",
      "[10,  1800] loss: 0.129\n",
      "[10,  1900] loss: 0.100\n",
      "[10,  2000] loss: 0.085\n",
      "[10,  2100] loss: 0.284\n",
      "[10,  2200] loss: 0.121\n",
      "[10,  2300] loss: 0.159\n",
      "[10,  2400] loss: 0.220\n",
      "[10,  2500] loss: 0.154\n",
      "[10,  2600] loss: 0.094\n",
      "[10,  2700] loss: 0.153\n",
      "[10,  2800] loss: 0.061\n",
      "[10,  2900] loss: 0.036\n",
      "[10,  3000] loss: 0.053\n",
      "[10,  3100] loss: 0.178\n",
      "[10,  3200] loss: 0.184\n",
      "[10,  3300] loss: 0.127\n",
      "[10,  3400] loss: 0.151\n",
      "[10,  3500] loss: 0.174\n",
      "[11,   100] loss: 0.149\n",
      "[11,   200] loss: 0.148\n",
      "[11,   300] loss: 0.246\n",
      "[11,   400] loss: 0.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11,   500] loss: 0.243\n",
      "[11,   600] loss: 0.161\n",
      "[11,   700] loss: 0.106\n",
      "[11,   800] loss: 0.229\n",
      "[11,   900] loss: 0.088\n",
      "[11,  1000] loss: 0.166\n",
      "[11,  1100] loss: 0.179\n",
      "[11,  1200] loss: 0.087\n",
      "[11,  1300] loss: 0.129\n",
      "[11,  1400] loss: 0.124\n",
      "[11,  1500] loss: 0.180\n",
      "[11,  1600] loss: 0.186\n",
      "[11,  1700] loss: 0.213\n",
      "[11,  1800] loss: 0.124\n",
      "[11,  1900] loss: 0.118\n",
      "[11,  2000] loss: 0.069\n",
      "[11,  2100] loss: 0.277\n",
      "[11,  2200] loss: 0.132\n",
      "[11,  2300] loss: 0.162\n",
      "[11,  2400] loss: 0.217\n",
      "[11,  2500] loss: 0.142\n",
      "[11,  2600] loss: 0.086\n",
      "[11,  2700] loss: 0.153\n",
      "[11,  2800] loss: 0.057\n",
      "[11,  2900] loss: 0.024\n",
      "[11,  3000] loss: 0.049\n",
      "[11,  3100] loss: 0.195\n",
      "[11,  3200] loss: 0.292\n",
      "[11,  3300] loss: 0.166\n",
      "[11,  3400] loss: 0.136\n",
      "[11,  3500] loss: 0.184\n",
      "[12,   100] loss: 0.092\n",
      "[12,   200] loss: 0.161\n",
      "[12,   300] loss: 0.229\n",
      "[12,   400] loss: 0.369\n",
      "[12,   500] loss: 0.240\n",
      "[12,   600] loss: 0.182\n",
      "[12,   700] loss: 0.107\n",
      "[12,   800] loss: 0.193\n",
      "[12,   900] loss: 0.090\n",
      "[12,  1000] loss: 0.164\n",
      "[12,  1100] loss: 0.189\n",
      "[12,  1200] loss: 0.075\n",
      "[12,  1300] loss: 0.187\n",
      "[12,  1400] loss: 0.156\n",
      "[12,  1500] loss: 0.235\n",
      "[12,  1600] loss: 0.172\n",
      "[12,  1700] loss: 0.225\n",
      "[12,  1800] loss: 0.124\n",
      "[12,  1900] loss: 0.105\n",
      "[12,  2000] loss: 0.073\n",
      "[12,  2100] loss: 0.298\n",
      "[12,  2200] loss: 0.141\n",
      "[12,  2300] loss: 0.176\n",
      "[12,  2400] loss: 0.218\n",
      "[12,  2500] loss: 0.156\n",
      "[12,  2600] loss: 0.097\n",
      "[12,  2700] loss: 0.154\n",
      "[12,  2800] loss: 0.057\n",
      "[12,  2900] loss: 0.020\n",
      "[12,  3000] loss: 0.058\n",
      "[12,  3100] loss: 0.179\n",
      "[12,  3200] loss: 0.176\n",
      "[12,  3300] loss: 0.131\n",
      "[12,  3400] loss: 0.073\n",
      "[12,  3500] loss: 0.125\n",
      "[13,   100] loss: 0.084\n",
      "[13,   200] loss: 0.135\n",
      "[13,   300] loss: 0.233\n",
      "[13,   400] loss: 0.292\n",
      "[13,   500] loss: 0.210\n",
      "[13,   600] loss: 0.137\n",
      "[13,   700] loss: 0.103\n",
      "[13,   800] loss: 0.233\n",
      "[13,   900] loss: 0.064\n",
      "[13,  1000] loss: 0.125\n",
      "[13,  1100] loss: 0.198\n",
      "[13,  1200] loss: 0.087\n",
      "[13,  1300] loss: 0.126\n",
      "[13,  1400] loss: 0.087\n",
      "[13,  1500] loss: 0.201\n",
      "[13,  1600] loss: 0.142\n",
      "[13,  1700] loss: 0.203\n",
      "[13,  1800] loss: 0.135\n",
      "[13,  1900] loss: 0.092\n",
      "[13,  2000] loss: 0.064\n",
      "[13,  2100] loss: 0.268\n",
      "[13,  2200] loss: 0.132\n",
      "[13,  2300] loss: 0.166\n",
      "[13,  2400] loss: 0.216\n",
      "[13,  2500] loss: 0.142\n",
      "[13,  2600] loss: 0.068\n",
      "[13,  2700] loss: 0.138\n",
      "[13,  2800] loss: 0.056\n",
      "[13,  2900] loss: 0.019\n",
      "[13,  3000] loss: 0.045\n",
      "[13,  3100] loss: 0.158\n",
      "[13,  3200] loss: 0.183\n",
      "[13,  3300] loss: 0.123\n",
      "[13,  3400] loss: 0.053\n",
      "[13,  3500] loss: 0.118\n",
      "[14,   100] loss: 0.072\n",
      "[14,   200] loss: 0.137\n",
      "[14,   300] loss: 0.240\n",
      "[14,   400] loss: 0.333\n",
      "[14,   500] loss: 0.225\n",
      "[14,   600] loss: 0.116\n",
      "[14,   700] loss: 0.090\n",
      "[14,   800] loss: 0.346\n",
      "[14,   900] loss: 0.069\n",
      "[14,  1000] loss: 0.169\n",
      "[14,  1100] loss: 0.208\n",
      "[14,  1200] loss: 0.057\n",
      "[14,  1300] loss: 0.163\n",
      "[14,  1400] loss: 0.099\n",
      "[14,  1500] loss: 0.112\n",
      "[14,  1600] loss: 0.167\n",
      "[14,  1700] loss: 0.191\n",
      "[14,  1800] loss: 0.121\n",
      "[14,  1900] loss: 0.099\n",
      "[14,  2000] loss: 0.049\n",
      "[14,  2100] loss: 0.305\n",
      "[14,  2200] loss: 0.196\n",
      "[14,  2300] loss: 0.175\n",
      "[14,  2400] loss: 0.203\n",
      "[14,  2500] loss: 0.149\n",
      "[14,  2600] loss: 0.101\n",
      "[14,  2700] loss: 0.166\n",
      "[14,  2800] loss: 0.052\n",
      "[14,  2900] loss: 0.027\n",
      "[14,  3000] loss: 0.051\n",
      "[14,  3100] loss: 0.137\n",
      "[14,  3200] loss: 0.158\n",
      "[14,  3300] loss: 0.115\n",
      "[14,  3400] loss: 0.038\n",
      "[14,  3500] loss: 0.090\n",
      "[15,   100] loss: 0.059\n",
      "[15,   200] loss: 0.129\n",
      "[15,   300] loss: 0.252\n",
      "[15,   400] loss: 0.352\n",
      "[15,   500] loss: 0.209\n",
      "[15,   600] loss: 0.233\n",
      "[15,   700] loss: 0.148\n",
      "[15,   800] loss: 0.211\n",
      "[15,   900] loss: 0.085\n",
      "[15,  1000] loss: 0.128\n",
      "[15,  1100] loss: 0.180\n",
      "[15,  1200] loss: 0.046\n",
      "[15,  1300] loss: 0.198\n",
      "[15,  1400] loss: 0.089\n",
      "[15,  1500] loss: 0.177\n",
      "[15,  1600] loss: 0.154\n",
      "[15,  1700] loss: 0.229\n",
      "[15,  1800] loss: 0.107\n",
      "[15,  1900] loss: 0.087\n",
      "[15,  2000] loss: 0.054\n",
      "[15,  2100] loss: 0.243\n",
      "[15,  2200] loss: 0.106\n",
      "[15,  2300] loss: 0.139\n",
      "[15,  2400] loss: 0.221\n",
      "[15,  2500] loss: 0.149\n",
      "[15,  2600] loss: 0.080\n",
      "[15,  2700] loss: 0.144\n",
      "[15,  2800] loss: 0.065\n",
      "[15,  2900] loss: 0.051\n",
      "[15,  3000] loss: 0.064\n",
      "[15,  3100] loss: 0.123\n",
      "[15,  3200] loss: 0.160\n",
      "[15,  3300] loss: 0.108\n",
      "[15,  3400] loss: 0.041\n",
      "[15,  3500] loss: 0.173\n",
      "[16,   100] loss: 0.063\n",
      "[16,   200] loss: 0.132\n",
      "[16,   300] loss: 0.213\n",
      "[16,   400] loss: 0.289\n",
      "[16,   500] loss: 0.233\n",
      "[16,   600] loss: 0.154\n",
      "[16,   700] loss: 0.070\n",
      "[16,   800] loss: 0.204\n",
      "[16,   900] loss: 0.100\n",
      "[16,  1000] loss: 0.103\n",
      "[16,  1100] loss: 0.183\n",
      "[16,  1200] loss: 0.057\n",
      "[16,  1300] loss: 0.161\n",
      "[16,  1400] loss: 0.117\n",
      "[16,  1500] loss: 0.111\n",
      "[16,  1600] loss: 0.135\n",
      "[16,  1700] loss: 0.215\n",
      "[16,  1800] loss: 0.119\n",
      "[16,  1900] loss: 0.090\n",
      "[16,  2000] loss: 0.049\n",
      "[16,  2100] loss: 0.246\n",
      "[16,  2200] loss: 0.107\n",
      "[16,  2300] loss: 0.160\n",
      "[16,  2400] loss: 0.183\n",
      "[16,  2500] loss: 0.140\n",
      "[16,  2600] loss: 0.073\n",
      "[16,  2700] loss: 0.171\n",
      "[16,  2800] loss: 0.058\n",
      "[16,  2900] loss: 0.060\n",
      "[16,  3000] loss: 0.041\n",
      "[16,  3100] loss: 0.140\n",
      "[16,  3200] loss: 0.142\n",
      "[16,  3300] loss: 0.115\n",
      "[16,  3400] loss: 0.027\n",
      "[16,  3500] loss: 0.145\n",
      "[17,   100] loss: 0.053\n",
      "[17,   200] loss: 0.115\n",
      "[17,   300] loss: 0.184\n",
      "[17,   400] loss: 0.271\n",
      "[17,   500] loss: 0.301\n",
      "[17,   600] loss: 0.151\n",
      "[17,   700] loss: 0.063\n",
      "[17,   800] loss: 0.173\n",
      "[17,   900] loss: 0.072\n",
      "[17,  1000] loss: 0.118\n",
      "[17,  1100] loss: 0.166\n",
      "[17,  1200] loss: 0.068\n",
      "[17,  1300] loss: 0.128\n",
      "[17,  1400] loss: 0.170\n",
      "[17,  1500] loss: 0.127\n",
      "[17,  1600] loss: 0.144\n",
      "[17,  1700] loss: 0.189\n",
      "[17,  1800] loss: 0.112\n",
      "[17,  1900] loss: 0.091\n",
      "[17,  2000] loss: 0.043\n",
      "[17,  2100] loss: 0.236\n",
      "[17,  2200] loss: 0.078\n",
      "[17,  2300] loss: 0.126\n",
      "[17,  2400] loss: 0.207\n",
      "[17,  2500] loss: 0.116\n",
      "[17,  2600] loss: 0.080\n",
      "[17,  2700] loss: 0.109\n",
      "[17,  2800] loss: 0.032\n",
      "[17,  2900] loss: 0.021\n",
      "[17,  3000] loss: 0.047\n",
      "[17,  3100] loss: 0.143\n",
      "[17,  3200] loss: 0.113\n",
      "[17,  3300] loss: 0.098\n",
      "[17,  3400] loss: 0.057\n",
      "[17,  3500] loss: 0.097\n",
      "[18,   100] loss: 0.055\n",
      "[18,   200] loss: 0.126\n",
      "[18,   300] loss: 0.162\n",
      "[18,   400] loss: 0.205\n",
      "[18,   500] loss: 0.222\n",
      "[18,   600] loss: 0.087\n",
      "[18,   700] loss: 0.060\n",
      "[18,   800] loss: 0.148\n",
      "[18,   900] loss: 0.092\n",
      "[18,  1000] loss: 0.106\n",
      "[18,  1100] loss: 0.204\n",
      "[18,  1200] loss: 0.047\n",
      "[18,  1300] loss: 0.174\n",
      "[18,  1400] loss: 0.228\n",
      "[18,  1500] loss: 0.328\n",
      "[18,  1600] loss: 0.151\n",
      "[18,  1700] loss: 0.206\n",
      "[18,  1800] loss: 0.119\n",
      "[18,  1900] loss: 0.095\n",
      "[18,  2000] loss: 0.068\n",
      "[18,  2100] loss: 0.268\n",
      "[18,  2200] loss: 0.118\n",
      "[18,  2300] loss: 0.160\n",
      "[18,  2400] loss: 0.194\n",
      "[18,  2500] loss: 0.124\n",
      "[18,  2600] loss: 0.086\n",
      "[18,  2700] loss: 0.138\n",
      "[18,  2800] loss: 0.043\n",
      "[18,  2900] loss: 0.034\n",
      "[18,  3000] loss: 0.068\n",
      "[18,  3100] loss: 0.119\n",
      "[18,  3200] loss: 0.177\n",
      "[18,  3300] loss: 0.071\n",
      "[18,  3400] loss: 0.026\n",
      "[18,  3500] loss: 0.074\n",
      "[19,   100] loss: 0.028\n",
      "[19,   200] loss: 0.105\n",
      "[19,   300] loss: 0.182\n",
      "[19,   400] loss: 0.237\n",
      "[19,   500] loss: 0.222\n",
      "[19,   600] loss: 0.089\n",
      "[19,   700] loss: 0.064\n",
      "[19,   800] loss: 0.205\n",
      "[19,   900] loss: 0.053\n",
      "[19,  1000] loss: 0.093\n",
      "[19,  1100] loss: 0.168\n",
      "[19,  1200] loss: 0.074\n",
      "[19,  1300] loss: 0.073\n",
      "[19,  1400] loss: 0.125\n",
      "[19,  1500] loss: 0.125\n",
      "[19,  1600] loss: 0.115\n",
      "[19,  1700] loss: 0.209\n",
      "[19,  1800] loss: 0.126\n",
      "[19,  1900] loss: 0.083\n",
      "[19,  2000] loss: 0.047\n",
      "[19,  2100] loss: 0.278\n",
      "[19,  2200] loss: 0.101\n",
      "[19,  2300] loss: 0.131\n",
      "[19,  2400] loss: 0.209\n",
      "[19,  2500] loss: 0.168\n",
      "[19,  2600] loss: 0.138\n",
      "[19,  2700] loss: 0.145\n",
      "[19,  2800] loss: 0.043\n",
      "[19,  2900] loss: 0.032\n",
      "[19,  3000] loss: 0.050\n",
      "[19,  3100] loss: 0.105\n",
      "[19,  3200] loss: 0.125\n",
      "[19,  3300] loss: 0.075\n",
      "[19,  3400] loss: 0.011\n",
      "[19,  3500] loss: 0.108\n",
      "[20,   100] loss: 0.051\n",
      "[20,   200] loss: 0.119\n",
      "[20,   300] loss: 0.138\n",
      "[20,   400] loss: 0.197\n",
      "[20,   500] loss: 0.237\n",
      "[20,   600] loss: 0.173\n",
      "[20,   700] loss: 0.042\n",
      "[20,   800] loss: 0.163\n",
      "[20,   900] loss: 0.058\n",
      "[20,  1000] loss: 0.105\n",
      "[20,  1100] loss: 0.129\n",
      "[20,  1200] loss: 0.027\n",
      "[20,  1300] loss: 0.103\n",
      "[20,  1400] loss: 0.084\n",
      "[20,  1500] loss: 0.091\n",
      "[20,  1600] loss: 0.125\n",
      "[20,  1700] loss: 0.182\n",
      "[20,  1800] loss: 0.096\n",
      "[20,  1900] loss: 0.103\n",
      "[20,  2000] loss: 0.040\n",
      "[20,  2100] loss: 0.232\n",
      "[20,  2200] loss: 0.098\n",
      "[20,  2300] loss: 0.122\n",
      "[20,  2400] loss: 0.231\n",
      "[20,  2500] loss: 0.119\n",
      "[20,  2600] loss: 0.072\n",
      "[20,  2700] loss: 0.119\n",
      "[20,  2800] loss: 0.024\n",
      "[20,  2900] loss: 0.015\n",
      "[20,  3000] loss: 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,  3100] loss: 0.078\n",
      "[20,  3200] loss: 0.097\n",
      "[20,  3300] loss: 0.078\n",
      "[20,  3400] loss: 0.008\n",
      "[20,  3500] loss: 0.089\n",
      "[21,   100] loss: 0.065\n",
      "[21,   200] loss: 0.127\n",
      "[21,   300] loss: 0.135\n",
      "[21,   400] loss: 0.214\n",
      "[21,   500] loss: 0.172\n",
      "[21,   600] loss: 0.066\n",
      "[21,   700] loss: 0.055\n",
      "[21,   800] loss: 0.146\n",
      "[21,   900] loss: 0.043\n",
      "[21,  1000] loss: 0.058\n",
      "[21,  1100] loss: 0.104\n",
      "[21,  1200] loss: 0.020\n",
      "[21,  1300] loss: 0.030\n",
      "[21,  1400] loss: 0.109\n",
      "[21,  1500] loss: 0.078\n",
      "[21,  1600] loss: 0.105\n",
      "[21,  1700] loss: 0.169\n",
      "[21,  1800] loss: 0.079\n",
      "[21,  1900] loss: 0.095\n",
      "[21,  2000] loss: 0.024\n",
      "[21,  2100] loss: 0.217\n",
      "[21,  2200] loss: 0.052\n",
      "[21,  2300] loss: 0.088\n",
      "[21,  2400] loss: 0.165\n",
      "[21,  2500] loss: 0.090\n",
      "[21,  2600] loss: 0.059\n",
      "[21,  2700] loss: 0.154\n",
      "[21,  2800] loss: 0.018\n",
      "[21,  2900] loss: 0.042\n",
      "[21,  3000] loss: 0.034\n",
      "[21,  3100] loss: 0.084\n",
      "[21,  3200] loss: 0.075\n",
      "[21,  3300] loss: 0.062\n",
      "[21,  3400] loss: 0.006\n",
      "[21,  3500] loss: 0.098\n",
      "[22,   100] loss: 0.018\n",
      "[22,   200] loss: 0.127\n",
      "[22,   300] loss: 0.176\n",
      "[22,   400] loss: 0.212\n",
      "[22,   500] loss: 0.167\n",
      "[22,   600] loss: 0.106\n",
      "[22,   700] loss: 0.119\n",
      "[22,   800] loss: 0.163\n",
      "[22,   900] loss: 0.042\n",
      "[22,  1000] loss: 0.057\n",
      "[22,  1100] loss: 0.118\n",
      "[22,  1200] loss: 0.037\n",
      "[22,  1300] loss: 0.126\n",
      "[22,  1400] loss: 0.085\n",
      "[22,  1500] loss: 0.109\n",
      "[22,  1600] loss: 0.173\n",
      "[22,  1700] loss: 0.209\n",
      "[22,  1800] loss: 0.101\n",
      "[22,  1900] loss: 0.108\n",
      "[22,  2000] loss: 0.020\n",
      "[22,  2100] loss: 0.237\n",
      "[22,  2200] loss: 0.086\n",
      "[22,  2300] loss: 0.103\n",
      "[22,  2400] loss: 0.158\n",
      "[22,  2500] loss: 0.096\n",
      "[22,  2600] loss: 0.101\n",
      "[22,  2700] loss: 0.099\n",
      "[22,  2800] loss: 0.087\n",
      "[22,  2900] loss: 0.011\n",
      "[22,  3000] loss: 0.048\n",
      "[22,  3100] loss: 0.092\n",
      "[22,  3200] loss: 0.125\n",
      "[22,  3300] loss: 0.084\n",
      "[22,  3400] loss: 0.063\n",
      "[22,  3500] loss: 0.153\n",
      "[23,   100] loss: 0.036\n",
      "[23,   200] loss: 0.111\n",
      "[23,   300] loss: 0.159\n",
      "[23,   400] loss: 0.189\n",
      "[23,   500] loss: 0.169\n",
      "[23,   600] loss: 0.063\n",
      "[23,   700] loss: 0.030\n",
      "[23,   800] loss: 0.120\n",
      "[23,   900] loss: 0.060\n",
      "[23,  1000] loss: 0.055\n",
      "[23,  1100] loss: 0.106\n",
      "[23,  1200] loss: 0.029\n",
      "[23,  1300] loss: 0.176\n",
      "[23,  1400] loss: 0.215\n",
      "[23,  1500] loss: 0.081\n",
      "[23,  1600] loss: 0.156\n",
      "[23,  1700] loss: 0.167\n",
      "[23,  1800] loss: 0.119\n",
      "[23,  1900] loss: 0.082\n",
      "[23,  2000] loss: 0.096\n",
      "[23,  2100] loss: 0.216\n",
      "[23,  2200] loss: 0.124\n",
      "[23,  2300] loss: 0.136\n",
      "[23,  2400] loss: 0.222\n",
      "[23,  2500] loss: 0.109\n",
      "[23,  2600] loss: 0.162\n",
      "[23,  2700] loss: 0.134\n",
      "[23,  2800] loss: 0.016\n",
      "[23,  2900] loss: 0.039\n",
      "[23,  3000] loss: 0.033\n",
      "[23,  3100] loss: 0.072\n",
      "[23,  3200] loss: 0.149\n",
      "[23,  3300] loss: 0.048\n",
      "[23,  3400] loss: 0.011\n",
      "[23,  3500] loss: 0.050\n",
      "[24,   100] loss: 0.035\n",
      "[24,   200] loss: 0.111\n",
      "[24,   300] loss: 0.186\n",
      "[24,   400] loss: 0.140\n",
      "[24,   500] loss: 0.105\n",
      "[24,   600] loss: 0.123\n",
      "[24,   700] loss: 0.063\n",
      "[24,   800] loss: 0.127\n",
      "[24,   900] loss: 0.062\n",
      "[24,  1000] loss: 0.048\n",
      "[24,  1100] loss: 0.107\n",
      "[24,  1200] loss: 0.176\n",
      "[24,  1300] loss: 0.161\n",
      "[24,  1400] loss: 0.151\n",
      "[24,  1500] loss: 0.111\n",
      "[24,  1600] loss: 0.145\n",
      "[24,  1700] loss: 0.118\n",
      "[24,  1800] loss: 0.102\n",
      "[24,  1900] loss: 0.071\n",
      "[24,  2000] loss: 0.014\n",
      "[24,  2100] loss: 0.213\n",
      "[24,  2200] loss: 0.059\n",
      "[24,  2300] loss: 0.088\n",
      "[24,  2400] loss: 0.214\n",
      "[24,  2500] loss: 0.099\n",
      "[24,  2600] loss: 0.071\n",
      "[24,  2700] loss: 0.108\n",
      "[24,  2800] loss: 0.005\n",
      "[24,  2900] loss: 0.053\n",
      "[24,  3000] loss: 0.045\n",
      "[24,  3100] loss: 0.153\n",
      "[24,  3200] loss: 0.125\n",
      "[24,  3300] loss: 0.132\n",
      "[24,  3400] loss: 0.009\n",
      "[24,  3500] loss: 0.105\n",
      "[25,   100] loss: 0.049\n",
      "[25,   200] loss: 0.128\n",
      "[25,   300] loss: 0.141\n",
      "[25,   400] loss: 0.253\n",
      "[25,   500] loss: 0.120\n",
      "[25,   600] loss: 0.098\n",
      "[25,   700] loss: 0.044\n",
      "[25,   800] loss: 0.104\n",
      "[25,   900] loss: 0.033\n",
      "[25,  1000] loss: 0.040\n",
      "[25,  1100] loss: 0.101\n",
      "[25,  1200] loss: 0.032\n",
      "[25,  1300] loss: 0.069\n",
      "[25,  1400] loss: 0.176\n",
      "[25,  1500] loss: 0.073\n",
      "[25,  1600] loss: 0.093\n",
      "[25,  1700] loss: 0.148\n",
      "[25,  1800] loss: 0.114\n",
      "[25,  1900] loss: 0.064\n",
      "[25,  2000] loss: 0.100\n",
      "[25,  2100] loss: 0.181\n",
      "[25,  2200] loss: 0.038\n",
      "[25,  2300] loss: 0.088\n",
      "[25,  2400] loss: 0.214\n",
      "[25,  2500] loss: 0.132\n",
      "[25,  2600] loss: 0.087\n",
      "[25,  2700] loss: 0.117\n",
      "[25,  2800] loss: 0.047\n",
      "[25,  2900] loss: 0.032\n",
      "[25,  3000] loss: 0.036\n",
      "[25,  3100] loss: 0.077\n",
      "[25,  3200] loss: 0.063\n",
      "[25,  3300] loss: 0.041\n",
      "[25,  3400] loss: 0.016\n",
      "[25,  3500] loss: 0.049\n",
      "[26,   100] loss: 0.017\n",
      "[26,   200] loss: 0.095\n",
      "[26,   300] loss: 0.142\n",
      "[26,   400] loss: 0.101\n",
      "[26,   500] loss: 0.127\n",
      "[26,   600] loss: 0.107\n",
      "[26,   700] loss: 0.043\n",
      "[26,   800] loss: 0.090\n",
      "[26,   900] loss: 0.019\n",
      "[26,  1000] loss: 0.027\n",
      "[26,  1100] loss: 0.060\n",
      "[26,  1200] loss: 0.026\n",
      "[26,  1300] loss: 0.214\n",
      "[26,  1400] loss: 0.120\n",
      "[26,  1500] loss: 0.162\n",
      "[26,  1600] loss: 0.279\n",
      "[26,  1700] loss: 0.141\n",
      "[26,  1800] loss: 0.095\n",
      "[26,  1900] loss: 0.086\n",
      "[26,  2000] loss: 0.021\n",
      "[26,  2100] loss: 0.158\n",
      "[26,  2200] loss: 0.068\n",
      "[26,  2300] loss: 0.154\n",
      "[26,  2400] loss: 0.140\n",
      "[26,  2500] loss: 0.099\n",
      "[26,  2600] loss: 0.035\n",
      "[26,  2700] loss: 0.088\n",
      "[26,  2800] loss: 0.007\n",
      "[26,  2900] loss: 0.011\n",
      "[26,  3000] loss: 0.053\n",
      "[26,  3100] loss: 0.131\n",
      "[26,  3200] loss: 0.066\n",
      "[26,  3300] loss: 0.069\n",
      "[26,  3400] loss: 0.005\n",
      "[26,  3500] loss: 0.034\n",
      "[27,   100] loss: 0.026\n",
      "[27,   200] loss: 0.153\n",
      "[27,   300] loss: 0.079\n",
      "[27,   400] loss: 0.136\n",
      "[27,   500] loss: 0.080\n",
      "[27,   600] loss: 0.081\n",
      "[27,   700] loss: 0.066\n",
      "[27,   800] loss: 0.096\n",
      "[27,   900] loss: 0.044\n",
      "[27,  1000] loss: 0.019\n",
      "[27,  1100] loss: 0.075\n",
      "[27,  1200] loss: 0.019\n",
      "[27,  1300] loss: 0.055\n",
      "[27,  1400] loss: 0.129\n",
      "[27,  1500] loss: 0.036\n",
      "[27,  1600] loss: 0.056\n",
      "[27,  1700] loss: 0.177\n",
      "[27,  1800] loss: 0.083\n",
      "[27,  1900] loss: 0.074\n",
      "[27,  2000] loss: 0.060\n",
      "[27,  2100] loss: 0.147\n",
      "[27,  2200] loss: 0.053\n",
      "[27,  2300] loss: 0.119\n",
      "[27,  2400] loss: 0.157\n",
      "[27,  2500] loss: 0.060\n",
      "[27,  2600] loss: 0.067\n",
      "[27,  2700] loss: 0.111\n",
      "[27,  2800] loss: 0.011\n",
      "[27,  2900] loss: 0.045\n",
      "[27,  3000] loss: 0.039\n",
      "[27,  3100] loss: 0.115\n",
      "[27,  3200] loss: 0.067\n",
      "[27,  3300] loss: 0.060\n",
      "[27,  3400] loss: 0.033\n",
      "[27,  3500] loss: 0.105\n",
      "[28,   100] loss: 0.032\n",
      "[28,   200] loss: 0.083\n",
      "[28,   300] loss: 0.157\n",
      "[28,   400] loss: 0.110\n",
      "[28,   500] loss: 0.134\n",
      "[28,   600] loss: 0.108\n",
      "[28,   700] loss: 0.071\n",
      "[28,   800] loss: 0.126\n",
      "[28,   900] loss: 0.064\n",
      "[28,  1000] loss: 0.050\n",
      "[28,  1100] loss: 0.157\n",
      "[28,  1200] loss: 0.018\n",
      "[28,  1300] loss: 0.045\n",
      "[28,  1400] loss: 0.128\n",
      "[28,  1500] loss: 0.061\n",
      "[28,  1600] loss: 0.147\n",
      "[28,  1700] loss: 0.101\n",
      "[28,  1800] loss: 0.096\n",
      "[28,  1900] loss: 0.100\n",
      "[28,  2000] loss: 0.041\n",
      "[28,  2100] loss: 0.202\n",
      "[28,  2200] loss: 0.057\n",
      "[28,  2300] loss: 0.101\n",
      "[28,  2400] loss: 0.301\n",
      "[28,  2500] loss: 0.088\n",
      "[28,  2600] loss: 0.072\n",
      "[28,  2700] loss: 0.097\n",
      "[28,  2800] loss: 0.119\n",
      "[28,  2900] loss: 0.094\n",
      "[28,  3000] loss: 0.068\n",
      "[28,  3100] loss: 0.138\n",
      "[28,  3200] loss: 0.196\n",
      "[28,  3300] loss: 0.071\n",
      "[28,  3400] loss: 0.012\n",
      "[28,  3500] loss: 0.103\n",
      "[29,   100] loss: 0.075\n",
      "[29,   200] loss: 0.153\n",
      "[29,   300] loss: 0.168\n",
      "[29,   400] loss: 0.129\n",
      "[29,   500] loss: 0.125\n",
      "[29,   600] loss: 0.067\n",
      "[29,   700] loss: 0.041\n",
      "[29,   800] loss: 0.084\n",
      "[29,   900] loss: 0.044\n",
      "[29,  1000] loss: 0.023\n",
      "[29,  1100] loss: 0.123\n",
      "[29,  1200] loss: 0.026\n",
      "[29,  1300] loss: 0.037\n",
      "[29,  1400] loss: 0.105\n",
      "[29,  1500] loss: 0.075\n",
      "[29,  1600] loss: 0.071\n",
      "[29,  1700] loss: 0.169\n",
      "[29,  1800] loss: 0.099\n",
      "[29,  1900] loss: 0.078\n",
      "[29,  2000] loss: 0.029\n",
      "[29,  2100] loss: 0.153\n",
      "[29,  2200] loss: 0.041\n",
      "[29,  2300] loss: 0.081\n",
      "[29,  2400] loss: 0.174\n",
      "[29,  2500] loss: 0.061\n",
      "[29,  2600] loss: 0.064\n",
      "[29,  2700] loss: 0.068\n",
      "[29,  2800] loss: 0.005\n",
      "[29,  2900] loss: 0.007\n",
      "[29,  3000] loss: 0.053\n",
      "[29,  3100] loss: 0.084\n",
      "[29,  3200] loss: 0.673\n",
      "[29,  3300] loss: 0.229\n",
      "[29,  3400] loss: 0.067\n",
      "[29,  3500] loss: 0.126\n",
      "[30,   100] loss: 0.105\n",
      "[30,   200] loss: 0.187\n",
      "[30,   300] loss: 0.192\n",
      "[30,   400] loss: 0.206\n",
      "[30,   500] loss: 0.233\n",
      "[30,   600] loss: 0.130\n",
      "[30,   700] loss: 0.095\n",
      "[30,   800] loss: 0.251\n",
      "[30,   900] loss: 0.099\n",
      "[30,  1000] loss: 0.246\n",
      "[30,  1100] loss: 0.162\n",
      "[30,  1200] loss: 0.047\n",
      "[30,  1300] loss: 0.046\n",
      "[30,  1400] loss: 0.214\n",
      "[30,  1500] loss: 0.212\n",
      "[30,  1600] loss: 0.314\n",
      "[30,  1700] loss: 0.294\n",
      "[30,  1800] loss: 0.141\n",
      "[30,  1900] loss: 0.092\n",
      "[30,  2000] loss: 0.038\n",
      "[30,  2100] loss: 0.215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30,  2200] loss: 0.121\n",
      "[30,  2300] loss: 0.102\n",
      "[30,  2400] loss: 0.172\n",
      "[30,  2500] loss: 0.086\n",
      "[30,  2600] loss: 0.080\n",
      "[30,  2700] loss: 0.157\n",
      "[30,  2800] loss: 0.050\n",
      "[30,  2900] loss: 0.029\n",
      "[30,  3000] loss: 0.054\n",
      "[30,  3100] loss: 0.111\n",
      "[30,  3200] loss: 0.096\n",
      "[30,  3300] loss: 0.067\n",
      "[30,  3400] loss: 0.013\n",
      "[30,  3500] loss: 0.066\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "CrossEntropyloss = []\n",
    "epo = []\n",
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    Lloss = 0.0\n",
    "    epo.append(epoch)\n",
    "    for i in range(len(df)):\n",
    "        origpic = cv2.imread(df.iloc[i,0]) # 讀取圖片\n",
    "        xmin = df.iloc[i,4]\n",
    "        ymin = df.iloc[i,5]\n",
    "        xmax = df.iloc[i,6]\n",
    "        ymax = df.iloc[i,7]\n",
    "        origpic = origpic[ymin:ymax,xmin:xmax]#框框大小\n",
    "        pic = cv2.cvtColor(origpic, cv2.COLOR_BGR2RGB) #色彩空間轉換opencv預設為:(BGR)=>RGB\n",
    "        pic = cv2.resize(origpic,(32,32)) #Resize(32*32*3)\n",
    "        to_tensor = ToTensor()#維度轉換(3*32*32)\n",
    "        pic = to_tensor(pic).unsqueeze(0) #(1*3*32*32)\n",
    "        optimizer.zero_grad() # 初始化 Gradient\n",
    "        output = cnn(pic) #開始訓練\n",
    "        if df.iloc[i,3]=='good':\n",
    "            label = torch.Tensor([0]).long()\n",
    "        elif df.iloc[i,3]=='bad':\n",
    "            label = torch.Tensor([1]).long()\n",
    "        elif df.iloc[i,3]=='none':\n",
    "            label = torch.Tensor([2]).long()\n",
    "        _, predicted = torch.max(output, 1)#出來兩個東西只要後面predict\n",
    "        loss = criterion(output,label) # Calculate Loss\n",
    "        #loss = loss*weight\n",
    "        loss.backward()  #Back propagation\n",
    "        optimizer.step() # Update the parameters\n",
    "        running_loss+=loss.item() # accumulate loss\n",
    "        Lloss+=loss.item()\n",
    "        if i%100==99:\n",
    "            print('[%d, %5d] loss: %.3f'%(epoch+1,i+1,running_loss/100))\n",
    "            running_loss = 0.0\n",
    "    CrossEntropyloss.append(Lloss/100)\n",
    "    \n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(df)):\n",
    "#             origpic = cv2.imread(df.iloc[i,0])\n",
    "#             xmin = df.iloc[i,4]\n",
    "#             ymin = df.iloc[i,5]\n",
    "#             xmax = df.iloc[i,6]\n",
    "#             ymax = df.iloc[i,7]\n",
    "#             origpic = origpic[ymin:ymax,xmin:xmax]\n",
    "#             pic = cv2.cvtColor(origpic, cv2.COLOR_BGR2RGB)\n",
    "#             pic = cv2.resize(origpic,(32,32))\n",
    "#             to_tensor = ToTensor()\n",
    "#             pic = to_tensor(pic).unsqueeze(0)\n",
    "#             optimizer.zero_grad()\n",
    "#             if df.iloc[i,3]=='good':\n",
    "#                 label = torch.Tensor([0]).long()\n",
    "#             elif df.iloc[i,3]=='bad':\n",
    "#                 label = torch.Tensor([1]).long()   \n",
    "#             elif df.iloc[i,3]=='none':\n",
    "#                 label = torch.Tensor([2]).long()\n",
    "#             output = cnn(img)\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             total += label.size(0)\n",
    "#             correct += (predicted == label).sum().item()\n",
    "#         trainingacc.append((100*correct / total))\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(df2)):\n",
    "#             origpic = cv2.imread(df2.iloc[i,0])\n",
    "#             xmin = df2.iloc[i,4]\n",
    "#             ymin = df2.iloc[i,5]\n",
    "#             xmax = df2.iloc[i,6]\n",
    "#             ymax = df2.iloc[i,7]\n",
    "#             origpic = origpic[ymin:ymax,xmin:xmax]\n",
    "#             pic = cv2.cvtColor(origpic, cv2.COLOR_BGR2RGB)\n",
    "#             pic = cv2.resize(origpic,(32,32))\n",
    "#             to_tensor = ToTensor()\n",
    "#             img = to_tensor(pic).unsqueeze(0)\n",
    "#             optimizer.zero_grad()\n",
    "#             if df2.iloc[i,3]=='good':\n",
    "#                 label = torch.Tensor([0]).long()\n",
    "#             elif df2.iloc[i,3]=='bad':\n",
    "#                 label = torch.Tensor([1]).long()   \n",
    "#             elif df2.iloc[i,3]=='none':\n",
    "#                 label = torch.Tensor([2]).long()\n",
    "#             output = cnn(pic)\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             total += label.size(0)\n",
    "#             correct += (predicted == label).sum().item()\n",
    "#         testacc.append((100*correct / total))\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epo,CrossEntropyloss)\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Numbers of epoch\")\n",
    "plt.ylabel(\"Cross Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) tensor([[-0.3836,  2.8662, -2.5233]])\n",
      "tensor([1]) tensor([[-4.9564,  7.2612, -2.6810]])\n",
      "tensor([1]) tensor([[-3.1181,  6.2658, -3.5852]])\n",
      "tensor([0]) tensor([[ 6.1408, -3.4486, -3.0551]])\n",
      "tensor([1]) tensor([[-0.2592,  2.7569, -2.5562]])\n",
      "tensor([0]) tensor([[ 5.1661, -5.0765, -1.4599]])\n",
      "tensor([1]) tensor([[ 0.5171,  0.6307, -1.2847]])\n",
      "tensor([1]) tensor([[ 0.0865,  0.9173, -1.1981]])\n",
      "tensor([2]) tensor([[ 0.9345, -4.5001,  3.6330]])\n",
      "tensor([1]) tensor([[-2.7286,  4.4457, -1.9224]])\n",
      "tensor([1]) tensor([[ 0.7743,  1.7549, -2.6548]])\n",
      "tensor([0]) tensor([[11.3460, -7.8502, -6.1781]])\n",
      "tensor([0]) tensor([[ 6.8427, -7.2541, -1.3863]])\n",
      "tensor([0]) tensor([[ 2.4791, -1.5521, -1.0123]])\n",
      "tensor([0]) tensor([[ 6.4847, -7.5498, -1.9714]])\n",
      "tensor([0]) tensor([[ 21.1598, -17.6929,  -9.3902]])\n",
      "tensor([0]) tensor([[ 1.8705, -1.1361, -1.2736]])\n",
      "tensor([1]) tensor([[ 0.1322,  3.2184, -3.4780]])\n",
      "tensor([0]) tensor([[ 6.8749, -4.8101, -3.9522]])\n",
      "tensor([0]) tensor([[ 3.4797, -3.8034, -0.1593]])\n",
      "tensor([1]) tensor([[ 2.4778,  3.2364, -6.3147]])\n",
      "tensor([0]) tensor([[ 19.1351, -16.7898,  -5.6985]])\n",
      "tensor([0]) tensor([[ 3.6953, -3.7719,  0.4354]])\n",
      "tensor([0]) tensor([[ 2.0120, -0.9859, -1.1545]])\n",
      "tensor([0]) tensor([[13.5035, -8.7932, -6.5781]])\n",
      "tensor([0]) tensor([[ 3.7510, -4.4449,  1.0535]])\n",
      "tensor([0]) tensor([[10.6362, -9.5933, -2.3524]])\n",
      "tensor([0]) tensor([[ 14.0517, -10.9180,  -5.1999]])\n",
      "tensor([0]) tensor([[12.4875, -7.3509, -6.7845]])\n",
      "tensor([0]) tensor([[ 8.7495, -7.1396, -3.0018]])\n",
      "tensor([0]) tensor([[ 8.1096, -4.6850, -4.1411]])\n",
      "tensor([1]) tensor([[-0.2504,  2.5335, -2.3319]])\n",
      "tensor([1]) tensor([[ 0.3299,  1.6129, -2.0928]])\n",
      "tensor([0]) tensor([[ 2.5424, -1.7605, -0.8904]])\n",
      "tensor([0]) tensor([[ 11.8406, -15.9722,   3.1743]])\n",
      "tensor([0]) tensor([[ 3.1449, -2.2682, -1.1661]])\n",
      "tensor([1]) tensor([[ 0.0448,  1.5545, -1.4801]])\n",
      "tensor([0]) tensor([[ 15.6338, -16.3467,  -1.9378]])\n",
      "tensor([0]) tensor([[ 13.9552, -10.2443,  -6.5588]])\n",
      "tensor([0]) tensor([[ 18.0327, -13.1651,  -8.8918]])\n",
      "tensor([0]) tensor([[ 7.7649, -7.2030, -2.3525]])\n",
      "tensor([0]) tensor([[ 4.7715, -3.5252, -1.6094]])\n",
      "tensor([0]) tensor([[ 9.5360, -7.7960, -3.4524]])\n",
      "tensor([0]) tensor([[ 2.6620, -4.1749,  1.0696]])\n",
      "tensor([1]) tensor([[ 0.5689,  2.5546, -2.9896]])\n",
      "tensor([0]) tensor([[ 11.1119, -11.1150,  -1.9824]])\n",
      "tensor([0]) tensor([[ 13.1196, -16.5500,   1.5280]])\n",
      "tensor([0]) tensor([[ 24.1438, -20.0553, -10.1374]])\n",
      "tensor([0]) tensor([[ 2.3337, -1.6409, -0.7059]])\n",
      "tensor([0]) tensor([[ 19.3406, -18.4774,  -6.7995]])\n",
      "tensor([0]) tensor([[ 2.0135, -2.1377,  0.2247]])\n",
      "tensor([0]) tensor([[ 21.7113, -18.6154,  -8.6782]])\n",
      "tensor([0]) tensor([[ 29.1876, -23.0943, -15.2917]])\n",
      "tensor([0]) tensor([[ 20.7612, -19.8431,  -4.7544]])\n",
      "tensor([0]) tensor([[ 24.2577, -19.1111, -11.4668]])\n",
      "tensor([0]) tensor([[ 26.2795, -18.9752, -15.0961]])\n",
      "tensor([0]) tensor([[ 29.2759, -24.9182, -12.9248]])\n",
      "tensor([0]) tensor([[ 23.8075, -16.8187, -13.5232]])\n",
      "tensor([0]) tensor([[ 22.7238, -17.4477, -10.0826]])\n",
      "tensor([0]) tensor([[ 28.4177, -21.8243, -15.1422]])\n",
      "tensor([0]) tensor([[ 22.3420, -17.5759, -10.6230]])\n",
      "tensor([0]) tensor([[ 17.8978, -13.1678, -10.0428]])\n",
      "tensor([0]) tensor([[ 16.9956, -15.4050,  -6.6102]])\n",
      "tensor([0]) tensor([[ 21.7661, -16.8920, -11.3923]])\n",
      "tensor([0]) tensor([[ 2.2827,  1.7524, -3.9428]])\n",
      "tensor([1]) tensor([[ 1.3483,  2.5127, -3.7241]])\n",
      "tensor([0]) tensor([[ 14.6037, -10.8798,  -7.2942]])\n",
      "tensor([0]) tensor([[ 9.6203, -7.2525, -5.2536]])\n",
      "tensor([0]) tensor([[11.9272, -8.8046, -5.0640]])\n",
      "tensor([0]) tensor([[ 5.7902, -8.4203,  2.6594]])\n",
      "tensor([0]) tensor([[ 10.8207, -10.2888,  -3.3667]])\n",
      "tensor([0]) tensor([[ 6.8188,  2.0199, -9.8057]])\n",
      "tensor([1]) tensor([[ 0.6216,  1.2700, -1.9209]])\n",
      "tensor([1]) tensor([[ 0.1968,  2.4538, -2.7300]])\n",
      "tensor([0]) tensor([[ 1.1226,  0.8093, -1.8035]])\n",
      "tensor([1]) tensor([[ 0.8880,  1.6447, -2.5386]])\n",
      "tensor([1]) tensor([[ 0.7630,  1.7473, -2.5154]])\n",
      "tensor([1]) tensor([[-0.7380,  4.0592, -3.5564]])\n",
      "tensor([0]) tensor([[ 22.2544, -18.6412,  -9.3436]])\n",
      "tensor([0]) tensor([[ 14.0812, -11.8830,  -6.0468]])\n",
      "tensor([0]) tensor([[ 3.8481, -1.0206, -2.8271]])\n",
      "tensor([0]) tensor([[ 2.3755,  0.2489, -2.4440]])\n",
      "tensor([0]) tensor([[ 13.6802, -10.8337,  -5.2315]])\n",
      "tensor([0]) tensor([[ 6.7688, -3.0568, -5.9177]])\n",
      "tensor([0]) tensor([[ 4.9744, -4.8950, -0.9265]])\n",
      "tensor([0]) tensor([[ 24.4457, -19.4505, -11.4924]])\n",
      "tensor([0]) tensor([[ 25.7401, -19.9408, -13.4573]])\n",
      "tensor([0]) tensor([[ 14.1075, -13.1302,  -5.2838]])\n",
      "tensor([0]) tensor([[ 2.3954, -3.2902,  0.9174]])\n",
      "tensor([0]) tensor([[ 3.0415, -3.0821, -0.4908]])\n",
      "tensor([0]) tensor([[ 25.1975, -22.0258, -11.1114]])\n",
      "tensor([0]) tensor([[ 2.1899, -0.9931, -0.6869]])\n",
      "tensor([0]) tensor([[ 6.8033, -4.2860, -3.4016]])\n",
      "tensor([0]) tensor([[ 4.9684, -3.9433, -2.1755]])\n",
      "tensor([1]) tensor([[ 0.1208,  2.9720, -2.9419]])\n",
      "tensor([0]) tensor([[ 1.4831,  0.1198, -1.4815]])\n",
      "tensor([1]) tensor([[-0.4957,  3.3026, -2.8245]])\n",
      "tensor([0]) tensor([[ 9.2533, -8.8247, -2.5730]])\n",
      "tensor([0]) tensor([[ 0.6537, -0.5928,  0.0564]])\n",
      "tensor([1]) tensor([[ 1.0620,  1.6382, -2.5046]])\n",
      "tensor([1]) tensor([[-0.0697,  2.6340, -2.5534]])\n",
      "tensor([0]) tensor([[12.5650, -9.8817, -5.4046]])\n",
      "tensor([0]) tensor([[ 9.8069, -8.0642, -3.6086]])\n",
      "tensor([0]) tensor([[ 5.8689, -8.6835,  2.8718]])\n",
      "tensor([1]) tensor([[-0.2186,  2.7899, -2.5798]])\n",
      "tensor([0]) tensor([[ 3.4950, -0.5841, -3.0456]])\n",
      "tensor([0]) tensor([[ 17.5547, -15.0246,  -7.3059]])\n",
      "tensor([0]) tensor([[ 14.4565, -14.1156,  -4.4023]])\n",
      "tensor([0]) tensor([[ 20.0567, -19.4089,  -5.1397]])\n",
      "tensor([0]) tensor([[ 14.7984, -14.3171,  -3.4483]])\n",
      "tensor([0]) tensor([[ 2.6560,  1.9285, -4.1456]])\n",
      "tensor([1]) tensor([[-1.4558,  3.1182, -1.6543]])\n",
      "tensor([1]) tensor([[-0.1630,  2.6668, -2.6152]])\n",
      "tensor([0]) tensor([[ 16.8930, -12.5564,  -7.0654]])\n",
      "tensor([0]) tensor([[ 15.0358, -10.6790,  -6.5245]])\n",
      "tensor([0]) tensor([[ 32.0548, -24.2473, -16.0596]])\n",
      "tensor([0]) tensor([[ 17.4865, -15.9670,  -2.7827]])\n",
      "tensor([0]) tensor([[ 20.7609, -19.9548,  -3.5675]])\n",
      "tensor([0]) tensor([[ 28.2834, -23.6625, -12.4359]])\n",
      "tensor([0]) tensor([[ 15.3412, -15.9146,  -2.1379]])\n",
      "tensor([0]) tensor([[ 17.1006, -12.2376,  -9.6737]])\n",
      "tensor([0]) tensor([[ 2.6731, -3.0631,  0.3608]])\n",
      "tensor([1]) tensor([[-1.6753,  3.5324, -1.4856]])\n",
      "tensor([0]) tensor([[ 2.2460, -0.0910, -2.2098]])\n",
      "tensor([0]) tensor([[ 3.2367, -1.9371, -1.4831]])\n",
      "tensor([0]) tensor([[ 15.1195, -11.7493,  -7.5350]])\n",
      "tensor([0]) tensor([[ 17.1881, -14.2575,  -6.5804]])\n",
      "tensor([0]) tensor([[ 14.2053, -10.0523,  -7.7285]])\n",
      "tensor([1]) tensor([[-0.2552,  3.3686, -2.8745]])\n",
      "tensor([0]) tensor([[ 2.5023, -1.1628, -1.5730]])\n",
      "tensor([0]) tensor([[ 2.8787, -5.6631,  2.3531]])\n",
      "tensor([0]) tensor([[ 17.5877, -17.5654,  -2.3733]])\n",
      "tensor([0]) tensor([[ 27.0252, -22.3012, -11.5202]])\n",
      "tensor([0]) tensor([[ 10.6004, -14.6692,   1.0816]])\n",
      "tensor([0]) tensor([[ 40.6339, -34.7094, -16.2150]])\n",
      "tensor([0]) tensor([[ 19.8275, -16.1259,  -8.3943]])\n",
      "tensor([0]) tensor([[ 21.6344, -16.9982,  -9.5338]])\n",
      "tensor([0]) tensor([[ 18.4328, -14.0045,  -8.9944]])\n",
      "tensor([0]) tensor([[ 3.7225, -6.8594,  3.0175]])\n",
      "tensor([0]) tensor([[ 29.2804, -25.2656, -11.8763]])\n",
      "tensor([0]) tensor([[ 11.2394, -14.5898,   2.2613]])\n",
      "tensor([0]) tensor([[11.3083, -8.8782, -5.4021]])\n",
      "tensor([1]) tensor([[-2.0810,  4.2982, -2.4597]])\n",
      "tensor([1]) tensor([[-3.3148,  4.1278, -0.8619]])\n",
      "tensor([0]) tensor([[ 1.7683,  0.8270, -2.5298]])\n",
      "tensor([0]) tensor([[ 14.1659, -11.7293,  -5.8175]])\n",
      "tensor([0]) tensor([[ 14.0640, -10.1507,  -6.2655]])\n",
      "tensor([0]) tensor([[ 14.1978, -13.9179,  -1.7563]])\n",
      "tensor([1]) tensor([[-0.6479,  3.5177, -3.0004]])\n",
      "tensor([0]) tensor([[11.0119, -9.3016, -4.3787]])\n",
      "tensor([0]) tensor([[ 8.9209, -5.4096, -5.3896]])\n",
      "tensor([0]) tensor([[ 15.5366, -10.7540,  -6.8164]])\n",
      "tensor([1]) tensor([[-0.2518,  2.5185, -2.3867]])\n",
      "tensor([0]) tensor([[ 3.4186, -2.1128, -1.3749]])\n",
      "tensor([0]) tensor([[ 3.8821, -4.4285, -0.1093]])\n",
      "tensor([0]) tensor([[ 3.1521, -5.6743,  2.8961]])\n",
      "tensor([0]) tensor([[ 2.7148, -3.1449,  0.8184]])\n",
      "tensor([0]) tensor([[ 2.1986, -3.8285,  1.7662]])\n",
      "tensor([0]) tensor([[ 4.9209, -4.3532, -1.4127]])\n",
      "tensor([0]) tensor([[ 13.2065, -13.0029,  -3.4201]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([[ 5.8123, -6.2458, -0.3824]])\n",
      "tensor([0]) tensor([[ 7.9823, -7.5215, -0.8741]])\n",
      "tensor([0]) tensor([[ 13.6023, -10.1838,  -5.8369]])\n",
      "tensor([0]) tensor([[ 4.3167, -3.4538, -1.4785]])\n",
      "tensor([0]) tensor([[ 15.2544, -11.3286,  -7.6623]])\n",
      "tensor([0]) tensor([[ 9.5168, -7.0088, -4.6622]])\n",
      "tensor([0]) tensor([[ 7.7525, -6.8644, -2.7475]])\n",
      "tensor([0]) tensor([[ 2.3896, -1.7585, -0.8728]])\n",
      "tensor([0]) tensor([[ 3.6534, -5.0372,  1.7965]])\n",
      "tensor([0]) tensor([[ 3.1553, -3.5383,  0.4421]])\n",
      "tensor([0]) tensor([[ 5.1649, -4.7660, -0.5764]])\n",
      "tensor([2]) tensor([[-3.2024, -0.3323,  3.1706]])\n",
      "tensor([0]) tensor([[ 2.6925, -1.7872, -1.0919]])\n",
      "tensor([0]) tensor([[ 3.7317, -4.1642, -0.2644]])\n",
      "tensor([0]) tensor([[11.0667, -8.3790, -3.5070]])\n",
      "tensor([0]) tensor([[ 3.2719, -3.4626,  0.4440]])\n",
      "tensor([0]) tensor([[ 3.8975, -2.7018, -2.0552]])\n",
      "tensor([0]) tensor([[10.7247, -8.5441, -4.0815]])\n",
      "tensor([1]) tensor([[ 0.0736,  2.1404, -2.3574]])\n",
      "tensor([1]) tensor([[ 0.7958,  3.0234, -3.5588]])\n",
      "tensor([1]) tensor([[-0.8756,  4.0669, -3.1876]])\n",
      "tensor([1]) tensor([[ 0.6625,  2.7308, -3.2486]])\n",
      "tensor([1]) tensor([[-2.6467,  2.1429,  0.5025]])\n",
      "tensor([1]) tensor([[-4.0655,  6.9912, -3.0928]])\n",
      "tensor([1]) tensor([[ 1.2757,  2.7461, -3.7969]])\n",
      "tensor([1]) tensor([[-1.5671,  5.2892, -3.3495]])\n",
      "tensor([0]) tensor([[ 7.2683, -6.0720, -2.1213]])\n",
      "tensor([0]) tensor([[11.8752, -9.4541, -4.3286]])\n",
      "tensor([0]) tensor([[ 17.3450, -14.2793,  -7.0348]])\n",
      "tensor([0]) tensor([[ 4.5057, -6.2101,  1.3858]])\n",
      "tensor([1]) tensor([[-1.5051,  4.9587, -3.1910]])\n",
      "tensor([1]) tensor([[ 0.6091,  2.4081, -2.9735]])\n",
      "tensor([0]) tensor([[ 3.6829, -2.6395, -1.3436]])\n",
      "tensor([0]) tensor([[ 8.1743, -9.9299, -0.3213]])\n",
      "tensor([0]) tensor([[ 13.1259, -10.4667,  -5.6857]])\n",
      "tensor([0]) tensor([[ 5.3213, -3.3015, -1.9931]])\n",
      "tensor([0]) tensor([[10.4677, -8.1029, -3.7991]])\n",
      "tensor([0]) tensor([[10.1752, -8.9190, -2.8870]])\n",
      "tensor([0]) tensor([[ 5.8407, -6.9104,  1.3670]])\n",
      "tensor([0]) tensor([[ 1.7844,  0.9324, -2.5009]])\n",
      "tensor([0]) tensor([[ 1.2834,  0.2451, -1.4659]])\n",
      "tensor([1]) tensor([[ 1.0098,  1.4845, -2.4502]])\n",
      "tensor([1]) tensor([[-0.8811,  1.1692, -0.1880]])\n",
      "tensor([0]) tensor([[ 5.9292, -4.0890, -2.2842]])\n",
      "tensor([0]) tensor([[ 6.8562, -9.7587,  3.2108]])\n",
      "tensor([1]) tensor([[-1.0231,  5.7842, -4.9342]])\n",
      "tensor([1]) tensor([[-0.7912,  3.6243, -2.8856]])\n",
      "tensor([2]) tensor([[  1.7104, -12.0599,  10.2271]])\n",
      "tensor([0]) tensor([[ 5.8538, -7.6832,  0.1292]])\n",
      "tensor([1]) tensor([[ 1.4546,  4.6200, -5.9326]])\n",
      "tensor([0]) tensor([[ 2.2052, -1.4505, -1.0064]])\n",
      "tensor([2]) tensor([[-0.8695, -3.2218,  4.3937]])\n",
      "tensor([1]) tensor([[-5.3446,  9.4648, -4.4079]])\n",
      "tensor([0]) tensor([[ 8.1873, -6.4654, -1.8397]])\n",
      "tensor([0]) tensor([[ 13.4009, -10.8056,  -5.4339]])\n",
      "tensor([0]) tensor([[ 8.4212, -7.0458, -4.0800]])\n",
      "tensor([1]) tensor([[ 0.2078,  3.0930, -3.3574]])\n",
      "tensor([0]) tensor([[ 31.7744, -24.4719, -15.5571]])\n",
      "tensor([1]) tensor([[-5.7746,  7.5969, -2.3997]])\n",
      "tensor([0]) tensor([[ 52.1869, -47.3739, -16.2453]])\n",
      "tensor([1]) tensor([[-2.5273,  4.9619, -2.1690]])\n",
      "tensor([1]) tensor([[ 0.7397,  5.1906, -5.9128]])\n",
      "tensor([0]) tensor([[ 5.0858, -6.5158,  0.8091]])\n",
      "tensor([0]) tensor([[ 24.3148, -22.2411,  -7.5094]])\n",
      "tensor([1]) tensor([[-0.3584,  3.0724, -2.8067]])\n",
      "tensor([0]) tensor([[ 50.9296, -46.2357, -16.0052]])\n",
      "tensor([0]) tensor([[ 52.8870, -47.3030, -18.5084]])\n",
      "tensor([0]) tensor([[ 26.2541, -20.4516, -11.1367]])\n",
      "tensor([0]) tensor([[ 17.6409, -11.0245,  -9.4202]])\n",
      "tensor([0]) tensor([[ 11.1608, -10.7758,  -3.3096]])\n",
      "tensor([1]) tensor([[-0.0634,  2.7638, -2.6730]])\n",
      "tensor([0]) tensor([[ 14.7705, -11.3003,  -5.0234]])\n",
      "tensor([0]) tensor([[ 25.7479, -19.2768, -13.9300]])\n",
      "tensor([0]) tensor([[ 21.4690, -16.4319, -10.7877]])\n",
      "tensor([1]) tensor([[ 0.4876,  2.3191, -2.7269]])\n",
      "tensor([0]) tensor([[ 26.4986, -20.7373, -10.9464]])\n",
      "tensor([0]) tensor([[10.0165, -8.5149, -2.5238]])\n",
      "tensor([0]) tensor([[ 19.8957, -20.7666,  -2.0045]])\n",
      "tensor([0]) tensor([[ 20.5190, -17.2353,  -6.9966]])\n",
      "tensor([0]) tensor([[ 16.7715, -13.7574,  -6.2068]])\n",
      "tensor([0]) tensor([[ 9.4698, -8.3185, -3.1755]])\n",
      "tensor([0]) tensor([[ 23.4259, -19.3448, -10.2210]])\n",
      "tensor([0]) tensor([[ 17.1395, -14.5946,  -7.3697]])\n",
      "tensor([0]) tensor([[ 22.1653, -18.7710,  -9.3992]])\n",
      "tensor([0]) tensor([[ 15.5743, -13.2422,  -5.7241]])\n",
      "tensor([0]) tensor([[ 4.8996, -4.0490, -1.2640]])\n",
      "tensor([0]) tensor([[ 17.7288, -15.8147,  -6.6349]])\n",
      "tensor([0]) tensor([[ 2.8393, -3.1092,  0.4791]])\n",
      "tensor([0]) tensor([[ 23.6865, -22.6921,  -5.3334]])\n",
      "tensor([0]) tensor([[ 1.7801, -0.8815, -0.9266]])\n",
      "tensor([0]) tensor([[ 1.6061, -0.9850, -0.9108]])\n",
      "tensor([0]) tensor([[ 6.3315, -7.6155,  0.1199]])\n",
      "tensor([0]) tensor([[ 1.8912, -2.4872,  0.9098]])\n",
      "tensor([0]) tensor([[ 14.2635, -11.3329,  -6.2822]])\n",
      "tensor([0]) tensor([[ 6.9058, -5.7190, -0.7313]])\n",
      "tensor([0]) tensor([[ 13.8749, -17.1548,  -0.6561]])\n",
      "tensor([0]) tensor([[ 23.5748, -23.1595,  -5.0474]])\n",
      "tensor([0]) tensor([[ 27.9507, -22.7101, -13.4573]])\n",
      "tensor([0]) tensor([[ 17.4729, -15.8545,  -7.8638]])\n",
      "tensor([0]) tensor([[ 22.1013, -17.2419,  -9.3116]])\n",
      "tensor([0]) tensor([[ 15.2232, -13.8381,  -5.5847]])\n",
      "tensor([0]) tensor([[ 9.4424, -6.0645, -4.2428]])\n",
      "tensor([0]) tensor([[ 20.3570, -13.1086, -11.0784]])\n",
      "tensor([1]) tensor([[-0.6565,  3.2459, -2.6474]])\n",
      "tensor([0]) tensor([[ 1.2775, -0.5044, -0.8076]])\n",
      "tensor([0]) tensor([[ 2.4268, -3.5471,  1.4370]])\n",
      "tensor([0]) tensor([[ 10.1174, -10.3280,  -2.1388]])\n",
      "tensor([2]) tensor([[ 0.7315, -5.0929,  3.5345]])\n",
      "tensor([1]) tensor([[ 0.6436,  2.5497, -3.1376]])\n",
      "tensor([0]) tensor([[ 15.7508, -13.3890,  -7.7584]])\n",
      "tensor([0]) tensor([[ 4.6131, -4.5153, -1.7408]])\n",
      "tensor([0]) tensor([[ 9.2431, -6.4030, -4.7078]])\n",
      "tensor([0]) tensor([[ 19.3806, -20.4676,  -1.7538]])\n",
      "tensor([1]) tensor([[-3.7236,  6.3092, -2.8717]])\n",
      "tensor([2]) tensor([[ 2.2446, -6.6539,  4.3064]])\n",
      "tensor([1]) tensor([[ 0.8855,  2.1030, -3.0305]])\n",
      "tensor([1]) tensor([[-0.4417,  3.9455, -3.8373]])\n",
      "tensor([0]) tensor([[ 13.3814, -12.2236,  -2.7142]])\n",
      "tensor([0]) tensor([[ 13.1916, -11.2026,  -4.6610]])\n",
      "tensor([0]) tensor([[11.2848, -9.0311, -4.2206]])\n",
      "tensor([0]) tensor([[ 2.2271,  0.0428, -2.0031]])\n",
      "tensor([0]) tensor([[ 3.8235, -1.5913, -2.3986]])\n",
      "tensor([0]) tensor([[ 5.1422, -6.5264,  1.1606]])\n",
      "tensor([0]) tensor([[ 12.2799, -10.0123,  -6.0048]])\n",
      "tensor([0]) tensor([[10.7130, -8.4024, -4.0658]])\n",
      "tensor([0]) tensor([[ 3.8319,  0.8163, -3.9302]])\n",
      "tensor([0]) tensor([[ 9.4515, -6.1352, -5.5356]])\n",
      "tensor([0]) tensor([[ 6.0181, -5.4545, -0.8654]])\n",
      "tensor([0]) tensor([[ 18.4444, -13.9251,  -8.6681]])\n",
      "tensor([0]) tensor([[ 8.4917, -9.4136,  0.4249]])\n",
      "tensor([0]) tensor([[ 2.7016, -1.5158, -1.1738]])\n",
      "tensor([0]) tensor([[ 8.8699, -6.3675, -4.5547]])\n",
      "tensor([1]) tensor([[-0.5082,  0.4055, -0.0050]])\n",
      "tensor([0]) tensor([[ 9.6638, -5.8964, -5.3122]])\n",
      "tensor([0]) tensor([[ 18.0665, -20.2795,  -0.3779]])\n",
      "tensor([0]) tensor([[ 8.1382, -9.7898,  1.7516]])\n",
      "tensor([0]) tensor([[ 4.8103, -1.7122, -3.4405]])\n",
      "tensor([0]) tensor([[ 2.9360, -1.5611, -1.3447]])\n",
      "tensor([2]) tensor([[-1.8737, -1.1867,  2.9181]])\n",
      "tensor([1]) tensor([[-0.4126,  3.5309, -3.2321]])\n",
      "tensor([0]) tensor([[ 9.5799, -3.0443, -8.9307]])\n",
      "tensor([0]) tensor([[ 4.5608, -4.4398, -0.1378]])\n",
      "tensor([1]) tensor([[ 1.1641,  2.3554, -3.1899]])\n",
      "tensor([2]) tensor([[-0.2363, -0.9050,  1.0177]])\n",
      "tensor([0]) tensor([[  9.7171, -12.2583,   1.0624]])\n",
      "tensor([0]) tensor([[ 4.8710, -3.1419, -1.1335]])\n",
      "tensor([0]) tensor([[ 8.8018, -7.6407, -2.2255]])\n",
      "tensor([1]) tensor([[ 0.4865,  1.9145, -2.3178]])\n",
      "tensor([0]) tensor([[ 8.7287, -4.0725, -6.7956]])\n",
      "tensor([0]) tensor([[ 5.8260, -6.0993,  0.0906]])\n",
      "tensor([0]) tensor([[ 16.7159, -10.7830,  -8.9736]])\n",
      "tensor([0]) tensor([[12.4336, -9.5087, -6.8032]])\n",
      "tensor([2]) tensor([[ 1.2490, -3.4789,  2.2205]])\n",
      "tensor([0]) tensor([[13.8748, -7.8291, -7.4585]])\n",
      "tensor([1]) tensor([[-1.0930,  4.5147, -3.7105]])\n",
      "tensor([0]) tensor([[ 18.4594, -17.2192,  -6.7921]])\n",
      "tensor([0]) tensor([[ 20.9017, -16.5253,  -9.4239]])\n",
      "tensor([0]) tensor([[ 13.9531, -10.7834,  -6.4270]])\n",
      "tensor([0]) tensor([[ 12.5761, -11.2992,  -4.2855]])\n",
      "tensor([0]) tensor([[ 26.3330, -22.7902,  -8.0829]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([[ 5.8477, -6.4841,  0.0648]])\n",
      "tensor([1]) tensor([[-1.9615,  4.7675, -2.9758]])\n",
      "tensor([2]) tensor([[ -4.5069, -22.8409,  26.8249]])\n",
      "tensor([0]) tensor([[ 24.0014, -19.1321, -11.6758]])\n",
      "tensor([0]) tensor([[ 20.5710, -16.9258,  -9.5944]])\n",
      "tensor([0]) tensor([[ 20.4128, -16.6443,  -9.5119]])\n",
      "tensor([2]) tensor([[ -2.5699, -24.1434,  26.3535]])\n",
      "tensor([1]) tensor([[-0.4927,  8.1876, -8.0668]])\n",
      "tensor([0]) tensor([[ 5.0561, -5.9387, -0.4203]])\n",
      "tensor([0]) tensor([[ 2.5108, -0.0608, -2.0409]])\n",
      "tensor([0]) tensor([[ 7.0096, -5.4009, -1.5529]])\n",
      "tensor([1]) tensor([[-3.1178,  5.6608, -2.5657]])\n",
      "tensor([1]) tensor([[ 0.6063,  0.9348, -1.6152]])\n",
      "tensor([0]) tensor([[ 6.0456, -2.1748, -4.4312]])\n",
      "tensor([2]) tensor([[-2.3624, -0.5033,  3.2181]])\n",
      "tensor([1]) tensor([[-3.7082,  6.6444, -3.2905]])\n",
      "tensor([0]) tensor([[ 1.8491, -1.0999, -0.6062]])\n",
      "tensor([0]) tensor([[ 14.8046, -11.8627,  -4.5442]])\n",
      "tensor([0]) tensor([[ 5.5994, -7.3815,  0.8295]])\n",
      "tensor([1]) tensor([[ 1.2342,  2.7554, -3.6227]])\n",
      "tensor([1]) tensor([[-0.6055,  4.4153, -3.7875]])\n",
      "tensor([0]) tensor([[13.0689, -7.8366, -6.8902]])\n",
      "tensor([1]) tensor([[ 1.2084,  2.1308, -3.0554]])\n",
      "tensor([0]) tensor([[ 9.0682, -7.2927, -2.8786]])\n",
      "tensor([0]) tensor([[ 24.6449, -21.8925,  -6.9604]])\n",
      "tensor([0]) tensor([[ 12.8465, -12.8835,  -1.8704]])\n",
      "tensor([0]) tensor([[ 6.8215, -3.8729, -3.0002]])\n",
      "tensor([0]) tensor([[ 23.0503, -19.1888, -10.3214]])\n",
      "tensor([0]) tensor([[ 8.6626, -9.2917,  0.7772]])\n",
      "tensor([0]) tensor([[ 8.9114, -5.6296, -4.7258]])\n",
      "tensor([1]) tensor([[-2.8316,  5.4158, -3.6264]])\n",
      "tensor([2]) tensor([[-0.6753,  0.2469,  0.2898]])\n",
      "tensor([0]) tensor([[ 15.5879, -12.1802,  -6.0837]])\n",
      "tensor([0]) tensor([[ 7.1365, -5.5744, -3.2533]])\n",
      "tensor([0]) tensor([[ 3.2682,  1.3964, -4.1883]])\n",
      "tensor([0]) tensor([[ 2.5436, -1.6064, -0.8799]])\n",
      "tensor([0]) tensor([[ 1.3350,  0.9241, -2.2502]])\n",
      "tensor([0]) tensor([[ 2.5330, -1.0490, -1.4559]])\n",
      "tensor([0]) tensor([[ 2.6303,  0.5885, -3.4095]])\n",
      "tensor([0]) tensor([[ 1.0868,  0.2385, -1.1702]])\n",
      "tensor([0]) tensor([[ 2.0395, -0.3945, -1.5120]])\n",
      "tensor([0]) tensor([[ 12.3036, -11.3946,  -3.3829]])\n",
      "tensor([0]) tensor([[ 4.9987, -7.0732,  2.3878]])\n",
      "tensor([0]) tensor([[ 5.7609, -8.1266,  1.8504]])\n",
      "tensor([0]) tensor([[ 3.5310, -4.2775,  0.8706]])\n",
      "tensor([0]) tensor([[ 3.1562, -5.0572,  1.8483]])\n",
      "tensor([0]) tensor([[ 6.1461, -4.7890, -2.6466]])\n",
      "tensor([0]) tensor([[12.8699, -8.7376, -7.8445]])\n",
      "tensor([0]) tensor([[10.6147, -8.6661, -5.0759]])\n",
      "tensor([1]) tensor([[-0.6972,  2.9018, -2.2681]])\n",
      "tensor([0]) tensor([[ 9.7404, -7.8044, -4.5085]])\n",
      "tensor([0]) tensor([[ 0.5200,  0.2897, -1.1896]])\n",
      "tensor([0]) tensor([[ 7.8793, -7.7534, -0.8923]])\n",
      "tensor([0]) tensor([[ 3.7414, -4.6747,  1.0167]])\n",
      "tensor([1]) tensor([[-2.7615,  4.9588, -2.3755]])\n",
      "tensor([0]) tensor([[ 22.5233, -19.4143,  -9.0548]])\n",
      "tensor([0]) tensor([[ 18.7746, -16.0544,  -7.7931]])\n",
      "tensor([0]) tensor([[ 10.0829, -10.8072,  -1.8943]])\n",
      "tensor([0]) tensor([[ 14.1505, -11.3118,  -7.5022]])\n",
      "tensor([0]) tensor([[ 5.6621, -4.0005, -2.7936]])\n",
      "tensor([1]) tensor([[-0.3562,  2.6173, -2.3473]])\n",
      "tensor([1]) tensor([[-0.0427,  2.8393, -2.7981]])\n",
      "tensor([0]) tensor([[ 3.3146, -3.3234, -0.0439]])\n",
      "tensor([0]) tensor([[ 4.5713, -1.7150, -2.6172]])\n",
      "tensor([1]) tensor([[ 0.0879,  2.7115, -2.8547]])\n",
      "tensor([0]) tensor([[ 6.6439, -7.3355,  0.8968]])\n",
      "tensor([0]) tensor([[ 7.0609, -4.3011, -3.4742]])\n",
      "tensor([1]) tensor([[ 0.8335,  1.6879, -2.4487]])\n",
      "tensor([0]) tensor([[ 7.2132, -5.9646, -1.7760]])\n",
      "tensor([0]) tensor([[ 10.7599, -11.6647,  -0.7762]])\n",
      "tensor([1]) tensor([[-0.3459,  4.0321, -3.7383]])\n",
      "tensor([2]) tensor([[-1.1820, -0.3454,  1.6498]])\n",
      "tensor([1]) tensor([[ 0.5810,  1.1937, -1.6809]])\n",
      "tensor([0]) tensor([[ 3.2987, -0.8874, -2.2245]])\n",
      "Accuracy of the network on the  test images: 90 %\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(df2)):\n",
    "        origpic = cv2.imread(df2.iloc[i,0])\n",
    "        xmin = df2.iloc[i,4]\n",
    "        ymin = df2.iloc[i,5]\n",
    "        xmax = df2.iloc[i,6]\n",
    "        ymax = df2.iloc[i,7]\n",
    "        origpic = origpic[ymin:ymax,xmin:xmax]\n",
    "        pic = cv2.cvtColor(origpic, cv2.COLOR_BGR2RGB)\n",
    "        pic = cv2.resize(origpic,(32,32))\n",
    "        to_tensor = ToTensor()\n",
    "        pic = to_tensor(pic).unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        if df2.iloc[i,3]=='good':\n",
    "            label = torch.Tensor([0]).long()\n",
    "        elif df2.iloc[i,3]=='bad':\n",
    "            label = torch.Tensor([1]).long()   \n",
    "        elif df2.iloc[i,3]=='none':\n",
    "            label = torch.Tensor([2]).long()\n",
    "        output = cnn(pic)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        print(predicted,output)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "#     print('Accuracy test images: %d %%' % (100*correct / total))\n",
    "#     plt.plot(epo[1:],CrossEntropyloss)\n",
    "#     plt.title(\"Learning curve\")\n",
    "#     plt.xlabel(\"Numbers of epoch\")\n",
    "#     plt.ylabel(\"Cross Entropy\")\n",
    "#     fig = plt.gcf()\n",
    "#     fig.savefig(\"learningcurve.png\")\n",
    "#     plt.clf()\n",
    "#     plt.plot(epo,trainingacc,label = \"Training acc\")\n",
    "#     plt.title(\"Training accuracy\")\n",
    "#     plt.xlabel(\"Numbers of epoch\")\n",
    "#     plt.ylabel(\"accuracy\")\n",
    "#     fig = plt.gcf()\n",
    "\n",
    "#     plt.plot(epo,testacc,label = \"Testing acc\")\n",
    "#     plt.title(\"Accruracy\")\n",
    "#     plt.xlabel(\"Numbers of epoch\")\n",
    "#     plt.ylabel(\"accuracy\")\n",
    "#     plt.legend(loc='upper right')\n",
    "#     fig = plt.gcf()\n",
    "#     fig.savefig(\"Accuracy.png\")\n",
    "\n",
    "#     print(\"Plot completed\")\n",
    "#     ans = {0:\"good\",1:\"bad\",2:\"none\"}\n",
    "#     class_correct = [0 for i in range(3)]\n",
    "#     class_total = [0 for i in range(3)]\n",
    "\n",
    "print('Accuracy: %d %%' % (100*correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
